{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /home/codespace/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: asyncio in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.4.3)\n",
      "Requirement already satisfied: requests_toolbelt in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: fabric-sdk-py in /usr/local/python/3.10.13/lib/python3.10/site-packages/fabric_sdk_py-0.9.0-py3.10.egg (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests_toolbelt) (2.31.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: aiogrpc>=1.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages/aiogrpc-1.8-py3.10.egg (from fabric-sdk-py) (1.8)\n",
      "Requirement already satisfied: cryptography>=1.9 in /usr/local/python/3.10.13/lib/python3.10/site-packages/cryptography-42.0.5-py3.10-linux-x86_64.egg (from fabric-sdk-py) (42.0.5)\n",
      "Requirement already satisfied: grpcio>=1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages/grpcio-1.62.1-py3.10-linux-x86_64.egg (from fabric-sdk-py) (1.62.1)\n",
      "Requirement already satisfied: hkdf>=0.0.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages/hkdf-0.0.3-py3.10.egg (from fabric-sdk-py) (0.0.3)\n",
      "Requirement already satisfied: lark-parser==0.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages/lark_parser-0.7.1-py3.10.egg (from fabric-sdk-py) (0.7.1)\n",
      "Requirement already satisfied: pycryptodomex>=3.4.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages/pycryptodomex-3.20.0-py3.10-linux-x86_64.egg (from fabric-sdk-py) (3.20.0)\n",
      "Requirement already satisfied: safe-pysha3>=1.0.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages/safe_pysha3-1.0.4-py3.10-linux-x86_64.egg (from fabric-sdk-py) (1.0.4)\n",
      "Requirement already satisfied: rx>=3.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages/Rx-3.2.0-py3.10.egg (from fabric-sdk-py) (3.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from fabric-sdk-py) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages/protobuf-5.26.1-py3.10.egg (from fabric-sdk-py) (5.26.1)\n",
      "Requirement already satisfied: couchdb>=1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages/CouchDB-1.2-py3.10.egg (from fabric-sdk-py) (1.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/codespace/.local/lib/python3.10/site-packages (from cryptography>=1.9->fabric-sdk-py) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.1->requests_toolbelt) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.1->requests_toolbelt) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.1->requests_toolbelt) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.1->requests_toolbelt) (2024.2.2)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.9->fabric-sdk-py) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio asyncio requests_toolbelt pandas fabric-sdk-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start HyperFabric Ledger using this command\n",
    "```bash\n",
    "$sh script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from requests_toolbelt.multipart.encoder import MultipartEncoder\n",
    "from typing import List\n",
    "import asyncio\n",
    "from hfc.fabric import Client\n",
    "import json\n",
    "from io import StringIO\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the Hyperledger Fabric Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Init client with profile=./network.json\n",
      "create org with name=org1.example.com\n",
      "create org with name=org2.example.com\n",
      "create ca with name=ca-org1\n",
      "create ca with name=ca-org2\n",
      "Import orderers = dict_keys(['orderer.example.com'])\n",
      "Import peers = dict_keys(['peer0.org1.example.com', 'peer0.org2.example.com'])\n",
      "New channel with name = mychannel\n"
     ]
    }
   ],
   "source": [
    "client = Client(net_profile=\"./network.json\")\n",
    "org1_admin = client.get_user(org_name='org1.example.com', name='Admin')\n",
    "client.new_channel('mychannel')\n",
    "channel_name = 'mychannel'\n",
    "chaincode_name = 'autodata_chaincode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Vars and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_SIZE = 33  # Number of rows per split\n",
    "JWT=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySW5mb3JtYXRpb24iOnsiaWQiOiI5ZGZhMjcwNi0zN2Q5LTRkNDEtODY1Ni04Njg3Zjg3NGRkZmMiLCJlbWFpbCI6ImRlbmlsYmhhdHQyMDA0QGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJwaW5fcG9saWN5Ijp7InJlZ2lvbnMiOlt7ImlkIjoiRlJBMSIsImRlc2lyZWRSZXBsaWNhdGlvbkNvdW50IjoxfSx7ImlkIjoiTllDMSIsImRlc2lyZWRSZXBsaWNhdGlvbkNvdW50IjoxfV0sInZlcnNpb24iOjF9LCJtZmFfZW5hYmxlZCI6ZmFsc2UsInN0YXR1cyI6IkFDVElWRSJ9LCJhdXRoZW50aWNhdGlvblR5cGUiOiJzY29wZWRLZXkiLCJzY29wZWRLZXlLZXkiOiI0ODY5NmE4MDJmYWQ1ODAxOGI2NyIsInNjb3BlZEtleVNlY3JldCI6IjRkYzEwZGI2MjZkODhlYTk5NzE0NmM3OGViNjdhODYxNTExNTc5OGIzOGIwNzkyMTc1ODI4N2Q3Njg1NzliNTIiLCJpYXQiOjE3MTE5OTI4OTB9.-htnX6xI8slvuJvRwW7OlfrTbk4xFtiAwPF-vrL66RY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "def wait(foo):\n",
    "    return loop.run_until_complete(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "def run_async(async_func):\n",
    "    \"\"\"\n",
    "    Run an async function synchronously and return the result.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    if loop.is_running():\n",
    "        return asyncio.ensure_future(async_func)\n",
    "    else:\n",
    "        return loop.run_until_complete(async_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_CSV and DF Utilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file into a DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def save_df(df: pd.DataFrame, file_path: str):\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def split_dataframe(df: pd.DataFrame, split_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a DataFrame into smaller DataFrames of specified size.\n",
    "    \"\"\"\n",
    "    return [df.iloc[i:i + split_size] for i in range(0, df.shape[0], split_size)]\n",
    "\n",
    "def dataframe_to_csv(df: pd.DataFrame, file_path: str):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def str_to_df(csv_content: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a CSV string to a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    csv_string_io = StringIO(csv_content)\n",
    "    return pd.read_csv(csv_string_io)\n",
    "\n",
    "def delete_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Delete file from the File System\n",
    "    \"\"\"\n",
    "    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_IPFS Utilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_ipfs(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Upload a file to IPFS via Pinata and return the IPFS hash.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    multipart_data = MultipartEncoder(\n",
    "        fields={\n",
    "            'file': (file_name, open(file_path, 'rb'), 'application/octet-stream'), \n",
    "            'pinataMetadata': '{\"name\": \"' + file_name + '\"}',\n",
    "            'pinataOptions': '{\"cidVersion\": 0}'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': multipart_data.content_type,\n",
    "        'Authorization': f'Bearer {JWT}'\n",
    "    }\n",
    "\n",
    "    response = requests.post('https://api.pinata.cloud/pinning/pinFileToIPFS', \n",
    "                             data=multipart_data, \n",
    "                             headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        ipfs_hash = response.json()[\"IpfsHash\"]\n",
    "        print(f\"File {file_name} successfully uploaded to IPFS with hash: {ipfs_hash}\")\n",
    "        return ipfs_hash\n",
    "    else:\n",
    "        print(f\"Failed to upload file {file_name}: {response.text}\")\n",
    "        return \"\"\n",
    "\n",
    "def fetch_from_ipfs(ipfs_hash: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches content from an IPFS hash using the Pinata gateway (or any other IPFS gateway).\n",
    "    \"\"\"\n",
    "    url = f\"https://gateway.pinata.cloud/ipfs/{ipfs_hash}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content.decode('utf-8')  # Assumes the content is text-based (e.g., CSV)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch file from IPFS with hash: {ipfs_hash}\")\n",
    "\n",
    "def retrieve_data_from_hashes(ipfs_hashes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves data from a list of IPFS hashes and concatenates it into a single DataFrame.\n",
    "    \"\"\"\n",
    "    dfs = []  # List to store individual data frames\n",
    "    for _hash in ipfs_hashes:\n",
    "        data = fetch_from_ipfs(_hash)\n",
    "        _df = str_to_df(data)\n",
    "        dfs.append(_df)  # Append the DataFrame to the list\n",
    "    \n",
    "    if dfs:  # Check if the list is not empty\n",
    "        df = pd.concat(dfs, ignore_index=True)  # Concatenate all DataFrames in the list\n",
    "    else:\n",
    "        df = pd.DataFrame()  # Return an empty DataFrame if no data was fetched\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hyperledger Chaincode/Smart Contract Utilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def store_hash_on_contract(user_address: str, ipfs_hash: str):\n",
    "    \"\"\"\n",
    "    Store a single IPFS hash on the blockchain via a smart contract function.\n",
    "    \"\"\"\n",
    "    args = [user_address, ipfs_hash]\n",
    "    response = await client.chaincode_invoke(\n",
    "        requestor=org1_admin,\n",
    "        channel_name=channel_name,\n",
    "        peers=['peer0.org1.example.com', 'peer0.org2.example.com'],\n",
    "        cc_name=chaincode_name,\n",
    "        fcn='AddIPFSHash',\n",
    "        args=args,\n",
    "        cc_pattern=None\n",
    "    )\n",
    "    print(f\"Added IPFS hash {ipfs_hash} on-chain\")\n",
    "    return response\n",
    "\n",
    "async def store_hashes_on_contract(user_address: str, ipfs_hashes: List[str]):\n",
    "    \"\"\"\n",
    "    Stores the list of IPFS hashes on a blockchain via a smart contract\n",
    "    \"\"\"\n",
    "\n",
    "    ipfs_hashes_str = json.dumps(ipfs_hashes)\n",
    "    args = [user_address, ipfs_hashes_str]\n",
    "    response = await client.chaincode_invoke(\n",
    "        requestor=org1_admin,\n",
    "        channel_name=channel_name,\n",
    "        peers=['peer0.org1.example.com', 'peer0.org2.example.com'],\n",
    "        cc_name=chaincode_name,\n",
    "        fcn='AddIPFSHashes',\n",
    "        args=args,\n",
    "        cc_pattern=None\n",
    "    )\n",
    "    print(f\"Added IPFS hash {ipfs_hashes} on-chain\")\n",
    "    return response\n",
    "    \n",
    "\n",
    "async def retrieve_hashes_from_contract(user_address: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the list of IPFS hashes stored in a smart contract for a user.\n",
    "    \"\"\"\n",
    "    # Implementation depends on the blockchain and contract\n",
    "    args = [user_address]\n",
    "    response = await client.chaincode_query(\n",
    "        requestor=org1_admin,\n",
    "        peers=['peer0.org1.example.com'],\n",
    "        channel_name=channel_name,\n",
    "        cc_name=chaincode_name,\n",
    "        fcn='GetIPFSHashes',\n",
    "        args=args\n",
    "    )\n",
    "    return json.loads(response)['ipfsHashes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def upload_user_data(file_path: str, user_address: str):\n",
    "    # Load CSV\n",
    "    df = load_csv(file_path)\n",
    "    \n",
    "    # Split DataFrame\n",
    "    dfs = split_dataframe(df, SPLIT_SIZE)\n",
    "    \n",
    "    ipfs_hashes = []\n",
    "    for i, split_df in enumerate(dfs, start=1):\n",
    "        # Save split DataFrame to CSV\n",
    "        split_file_path = f\"{user_address}_split_{i}.csv\"\n",
    "        dataframe_to_csv(split_df, split_file_path)\n",
    "        \n",
    "        # Upload to IPFS\n",
    "        ipfs_hash = upload_to_ipfs(split_file_path)\n",
    "        ipfs_hashes.append(ipfs_hash)\n",
    "        print(f\"Uploaded split {i} to IPFS with hash: {ipfs_hash}\")\n",
    "        delete_file(split_file_path)\n",
    "\n",
    "    # Store IPFS hashes on blockchain\n",
    "    await store_hashes_on_contract(user_address, ipfs_hashes)\n",
    "    \n",
    "    print(f\"Stored IPFS hashes on blockchain for user: {user_address}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_user_data(user_address: str):\n",
    "    ipfs_hashes = await retrieve_hashes_from_contract(user_address)\n",
    "    print(ipfs_hashes)\n",
    "    combined_df = retrieve_data_from_hashes(ipfs_hashes)\n",
    "    filename = f\"{user_address}_combined.csv\"\n",
    "    save_df(combined_df, filename)\n",
    "    print(f\"Data saved to {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_menu():\n",
    "    print(\"<\",\"=\"*5,\"MENU\",\"=\"*5,\">\")\n",
    "    print(\"Select an option:\")\n",
    "    print(\"1. Upload user data\")\n",
    "    print(\"2. Get user data\")\n",
    "    print(\"3. Exit\")\n",
    "    choice = input(\"Enter your choice (1/2/3): \")\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< ===== MENU ===== >\n",
      "Select an option:\n",
      "1. Upload user data\n",
      "2. Get user data\n",
      "3. Exit\n",
      "< ===== UPLOAD USER DATA ===== >\n",
      "File 0x123_split_1.csv successfully uploaded to IPFS with hash: QmNuUgVgcURXFWM6HRyAm9gC3ZQQ1MXSsEkCZRUcasdmVf\n",
      "Uploaded split 1 to IPFS with hash: QmNuUgVgcURXFWM6HRyAm9gC3ZQQ1MXSsEkCZRUcasdmVf\n",
      "File 0x123_split_2.csv successfully uploaded to IPFS with hash: QmfUkMrmkJdvw5KNVHYRusejFu9F9rLdoEoR3kWLcGwPgm\n",
      "Uploaded split 2 to IPFS with hash: QmfUkMrmkJdvw5KNVHYRusejFu9F9rLdoEoR3kWLcGwPgm\n",
      "File 0x123_split_3.csv successfully uploaded to IPFS with hash: QmUkcG4JdRhUhKajfBTjUwa2W8yKtwBdCyA9Bt3k37G9yd\n",
      "Uploaded split 3 to IPFS with hash: QmUkcG4JdRhUhKajfBTjUwa2W8yKtwBdCyA9Bt3k37G9yd\n",
      "Added IPFS hash ['QmNuUgVgcURXFWM6HRyAm9gC3ZQQ1MXSsEkCZRUcasdmVf', 'QmfUkMrmkJdvw5KNVHYRusejFu9F9rLdoEoR3kWLcGwPgm', 'QmUkcG4JdRhUhKajfBTjUwa2W8yKtwBdCyA9Bt3k37G9yd'] on-chain\n",
      "Stored IPFS hashes on blockchain for user: 0x123\n",
      "< ===== MENU ===== >\n",
      "Select an option:\n",
      "1. Upload user data\n",
      "2. Get user data\n",
      "3. Exit\n",
      "< ===== GET USER DATA ===== >\n",
      "['QmNuUgVgcURXFWM6HRyAm9gC3ZQQ1MXSsEkCZRUcasdmVf', 'QmfUkMrmkJdvw5KNVHYRusejFu9F9rLdoEoR3kWLcGwPgm', 'QmUkcG4JdRhUhKajfBTjUwa2W8yKtwBdCyA9Bt3k37G9yd']\n",
      "Data saved to 0x123_combined.csv.\n",
      "< ===== MENU ===== >\n",
      "Select an option:\n",
      "1. Upload user data\n",
      "2. Get user data\n",
      "3. Exit\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    while True:\n",
    "        choice = display_menu()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            print(\"<\",\"=\"*5,\"UPLOAD USER DATA\",\"=\"*5,\">\")\n",
    "            file_path = input(\"Enter the file path: \")\n",
    "            user_address = input(\"Enter the user address: \")\n",
    "            await upload_user_data(file_path, user_address)\n",
    "\n",
    "        elif choice == \"2\":\n",
    "            print(\"<\",\"=\"*5,\"GET USER DATA\",\"=\"*5,\">\")\n",
    "            user_address = input(\"Enter the user address: \")\n",
    "            await get_user_data(user_address)\n",
    "\n",
    "        elif choice == \"3\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice. Try again.\")\n",
    "        input(\"Enter any key to continue...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
